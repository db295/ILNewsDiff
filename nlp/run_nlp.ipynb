{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_nlp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vktpFVr7PLaY"
      },
      "source": [
        "## 1. Import Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elxv1xgsKbF-"
      },
      "source": [
        "Run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M9G6AB6KVdh",
        "outputId": "d9a5b1c1-ac5d-418c-baa3-8bf89c11ae70"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWjThh8lKiAh"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device.   \n",
        "Later, in our training loop, we will load data onto the device. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkK9G2PcKjEd",
        "outputId": "c757d08f-72b8-4ced-dfb5-153b9cc9cef0"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qqUI5MHLKI7",
        "outputId": "ba1ff8cd-23a4-49a5-987d-163d3df32801"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAcA9dM6PDEm"
      },
      "source": [
        "## 2. Parse\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SBtG_9iLRiz",
        "outputId": "66b12999-4cf7-4ac9-c9f1-19b7f5042e14"
      },
      "source": [
        "# Check that dataset file is available\n",
        "import glob\n",
        "\n",
        "version_files = glob.glob('./versions_*.csv')\n",
        "\n",
        "if len(version_files) == 0:\n",
        "  raise SystemError(\"Cant find any versions!!\")\n",
        "\n",
        "\n",
        "# TODO: get latest uploaded version..\n",
        "version_file = version_files[0]\n",
        "print(f\"Using - {version_file}\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using - ./versions_2.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t22D6PsjPUBz",
        "outputId": "137ca81a-fc08-4227-92f9-f77d9364e98a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(version_file)\n",
        "\n",
        "# Report the number of sentences.\n",
        "print(f'Number of sentences: {df.shape[0]}')\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences: 187531\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>id</th>\n",
              "      <th>article_id</th>\n",
              "      <th>article_source</th>\n",
              "      <th>url</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>hash</th>\n",
              "      <th>date_time</th>\n",
              "      <th>version</th>\n",
              "      <th>amount_of_words</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50269</th>\n",
              "      <td>50269</td>\n",
              "      <td>50269</td>\n",
              "      <td>50270</td>\n",
              "      <td>1.9812418</td>\n",
              "      <td>Haaretz</td>\n",
              "      <td>https://www.haaretz.co.il/blogs/shukifriedman/...</td>\n",
              "      <td>התורה כבסיס לגשר, לא כמצע לעימות בערינו</td>\n",
              "      <td>התורה, שאת קבלתה בהר סיני חוגגים הערב, מעבירה ...</td>\n",
              "      <td>06d21584b54fa2bb509d95059d8aa7cc1dff57f0bc6bef...</td>\n",
              "      <td>2021-05-18 07:30:01.872449</td>\n",
              "      <td>971</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86430</th>\n",
              "      <td>86430</td>\n",
              "      <td>86430</td>\n",
              "      <td>86431</td>\n",
              "      <td>1.9812409</td>\n",
              "      <td>Haaretz</td>\n",
              "      <td>https://www.haaretz.co.il/wellbeing/health-blo...</td>\n",
              "      <td>את מי זה מעניין אם הרופא יהודי או ערבי?</td>\n",
              "      <td>יהודים, מוסלמים ונוצרים ממשיכים להציל חיים בבי...</td>\n",
              "      <td>aeb2e49afca6e026a2a601f1806d55a3ac35bde7597250...</td>\n",
              "      <td>2021-05-21 03:57:02.565842</td>\n",
              "      <td>4889</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33111</th>\n",
              "      <td>33111</td>\n",
              "      <td>33111</td>\n",
              "      <td>33112</td>\n",
              "      <td>1.8706066</td>\n",
              "      <td>Haaretz</td>\n",
              "      <td>https://www.haaretz.co.il/sport/opinions/.prem...</td>\n",
              "      <td>דחיית האולימפיאדה היא הוכחה למי שטרם הבין: אנח...</td>\n",
              "      <td>רק שני מאורעות גרמו לשיבוש המשחקים האולימפיים ...</td>\n",
              "      <td>3c3aac99635d51fc2a41b0c8656fd80bf6bf260a4e09b5...</td>\n",
              "      <td>2021-04-21 20:06:03.386298</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95173</th>\n",
              "      <td>95173</td>\n",
              "      <td>95173</td>\n",
              "      <td>95174</td>\n",
              "      <td>1.9793247</td>\n",
              "      <td>Haaretz</td>\n",
              "      <td>https://www.haaretz.co.il/blogs/yasminelevi/BL...</td>\n",
              "      <td>יש מי שעושה במכנסיים ויש מי שמוכן לתפקיד</td>\n",
              "      <td>עבור ישראלים החרדים ממנהיג רעיל ומשולח כל רסן ...</td>\n",
              "      <td>2b9372c6e7910d29998d43807dcebc5cabf3cab0054ddf...</td>\n",
              "      <td>2021-05-21 20:58:02.136266</td>\n",
              "      <td>5865</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51475</th>\n",
              "      <td>51475</td>\n",
              "      <td>51475</td>\n",
              "      <td>51476</td>\n",
              "      <td>1.9812233</td>\n",
              "      <td>Haaretz</td>\n",
              "      <td>https://www.haaretz.co.il/blogs/anumuseum/BLOG...</td>\n",
              "      <td>מחקלאי-לאומי לדתי-רוחני: הטרנספורמציה של שבועות</td>\n",
              "      <td>בימים כתיקונם, הערב היתה לובשת ישראל חג ומגוון...</td>\n",
              "      <td>3892c946128aeaffb6c0e648d2f1a21b449ec72ed99bce...</td>\n",
              "      <td>2021-05-18 09:40:02.527090</td>\n",
              "      <td>1095</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156091</th>\n",
              "      <td>156091</td>\n",
              "      <td>156091</td>\n",
              "      <td>156092</td>\n",
              "      <td>1.9881510</td>\n",
              "      <td>Haaretz</td>\n",
              "      <td>https://www.haaretz.co.il/wellbeing/health-blo...</td>\n",
              "      <td>כדי להתגבר על הפחדים עליכם להשלים עמם</td>\n",
              "      <td>למרות המסרים שאנחנו מקבלים כל הזמן מהסביבה, הנ...</td>\n",
              "      <td>06e08185ef67d37b2e2dc2dfa63c0a2f8514d308ddccea...</td>\n",
              "      <td>2021-06-15 09:42:03.161347</td>\n",
              "      <td>4041</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9018</th>\n",
              "      <td>9018</td>\n",
              "      <td>9018</td>\n",
              "      <td>9019</td>\n",
              "      <td>https://news.walla.co.il/item/3418448</td>\n",
              "      <td>Walla</td>\n",
              "      <td>https://news.walla.co.il/item/3418448</td>\n",
              "      <td>השלג בדרך, וברשויות נערכים לסערה: \"מענה לכל תר...</td>\n",
              "      <td>לקראת המערכת החורפית הצפויה להתחיל היום, במשטר...</td>\n",
              "      <td>5edfaae81a41c477cb48f39ba054bc95faab301d39d6eb...</td>\n",
              "      <td>2021-02-16 15:15:52.699042</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151464</th>\n",
              "      <td>151464</td>\n",
              "      <td>151464</td>\n",
              "      <td>151465</td>\n",
              "      <td>1.9881331</td>\n",
              "      <td>Haaretz</td>\n",
              "      <td>https://www.haaretz.co.il/blogs/barrydanino/BL...</td>\n",
              "      <td>יומני היקר, חוזר מרעננה. ממתין עוד להבהרת הרב</td>\n",
              "      <td>שלוש רשימות דמיוניות מיומנו של תושב השומרון, ב...</td>\n",
              "      <td>fbfa19604d9c4173f9cdbed3078017d9ddb383ea719346...</td>\n",
              "      <td>2021-06-14 22:02:02.026999</td>\n",
              "      <td>3371</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55428</th>\n",
              "      <td>55428</td>\n",
              "      <td>55428</td>\n",
              "      <td>55429</td>\n",
              "      <td>1.9812418</td>\n",
              "      <td>Haaretz</td>\n",
              "      <td>https://www.haaretz.co.il/blogs/shukifriedman/...</td>\n",
              "      <td>התורה כבסיס לגשר, לא כמצע לעימות בערינו</td>\n",
              "      <td>התורה, שאת קבלתה בהר סיני חוגגים הערב, מעבירה ...</td>\n",
              "      <td>06d21584b54fa2bb509d95059d8aa7cc1dff57f0bc6bef...</td>\n",
              "      <td>2021-05-18 16:46:02.601055</td>\n",
              "      <td>1527</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132917</th>\n",
              "      <td>132917</td>\n",
              "      <td>132917</td>\n",
              "      <td>132918</td>\n",
              "      <td>1.9881463</td>\n",
              "      <td>Haaretz</td>\n",
              "      <td>https://www.haaretz.co.il/family/einattalmon/B...</td>\n",
              "      <td>נושא הלינה המשותפת מעורר שאלות רבות. הנה התשוב...</td>\n",
              "      <td>כיצד ליישם שינה בטוחה יחד עם התינוק, האם אימונ...</td>\n",
              "      <td>819c6aa49261ed78140dd6322bb5141da46d7ec101117c...</td>\n",
              "      <td>2021-06-13 01:00:01.864472</td>\n",
              "      <td>832</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0  Unnamed: 0.1      id  ... version amount_of_words label\n",
              "50269        50269         50269   50270  ...     971               7     1\n",
              "86430        86430         86430   86431  ...    4889               9     1\n",
              "33111        33111         33111   33112  ...       1              11     1\n",
              "95173        95173         95173   95174  ...    5865               8     1\n",
              "51475        51475         51475   51476  ...    1095               5     1\n",
              "156091      156091        156091  156092  ...    4041               7     1\n",
              "9018          9018          9018    9019  ...       2               8     1\n",
              "151464      151464        151464  151465  ...    3371               8     1\n",
              "55428        55428         55428   55429  ...    1527               7     1\n",
              "132917      132917        132917  132918  ...     832               9     1\n",
              "\n",
              "[10 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_--qeEcQxea",
        "outputId": "a9f715d4-dd16-4550-e968-b3a400f9e8ba"
      },
      "source": [
        "MAX_WORDS = df.amount_of_words.max()\n",
        "print(f\"Max words in sentence is - {MAX_WORDS}\")\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max words in sentence is - 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4Rw6qO8SIBZ"
      },
      "source": [
        "Let's extract the sentences and labels of our training set as numpy ndarrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qJO5qALSEd_"
      },
      "source": [
        "sentences = df.title.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id06SWSUUSll"
      },
      "source": [
        "# 3. Tokenization & Input Formatting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0rhPXDqUdLd"
      },
      "source": [
        "## 3.1. BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcwpkPcIUZou",
        "outputId": "5b748d58-ab70-4f4f-deb8-848790de4ad9"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIvfv_mbY6Y2"
      },
      "source": [
        "Let's apply the tokenizer to one sentence just to see the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVEWt2kLY5Nt",
        "outputId": "217137d9-cc43-4fd6-8d89-80adb54478ef"
      },
      "source": [
        "import random\n",
        "\n",
        "example_sentence = sentences[random.choice(range(len(sentences)))]\n",
        "# Print the original sentence.\n",
        "print(' Original: ', example_sentence)\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(example_sentence))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(example_sentence)))\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  גדעון סער: \"לא נצטרף לנתניהו ולא נתמוך בו\"\n",
            "Tokenized:  ['גדעון', 'סער', ':', '\"', 'לא', 'נצ', '##טרף', 'לנתניהו', 'ולא', 'נת', '##מוך', 'בו', '\"']\n",
            "Token IDs:  [10811, 16183, 30, 6, 1527, 2804, 4279, 26322, 1801, 1873, 2603, 1846, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BnDqNJOmR-o",
        "outputId": "b94c3aab-49ba-4582-b469-ab13b1ca4f12"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  קורונה? MeToo? כדורים? העיקר שהתחת שלי השתחרר!\n",
            "Token IDs: [2, 14508, 35, 11113, 18548, 35, 14726, 35, 5390, 8866, 1019, 2088, 2458, 5277, 5, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Mj8U0fm6Sn",
        "outputId": "42196bea-5d9f-47f5-f01e-ec8ca79b0fa6"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "MAX_LEN = 32\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 32 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCkAuHG0ns0e"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoM6r2Mfny5q"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oPwr7vBoLu5"
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ5-ph-CoUhW"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKvjUaNrYrL"
      },
      "source": [
        "##????"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZMgCqrFil4W",
        "outputId": "29f302b3-48e7-444d-f0da-eb2d4cc1b3b8"
      },
      "source": [
        "model = AutoModelForMaskedLM.from_pretrained(\"avichr/heBERT\")\n",
        "\n",
        "from transformers import pipeline\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model=\"avichr/heBERT\",\n",
        "    tokenizer=\"avichr/heBERT\"\n",
        ")\n",
        "\n",
        "options = fill_mask(\"הקורונה לקחה את [MASK] ולנו לא נשאר דבר.\")\n",
        "\n",
        "options"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.14047907292842865,\n",
              "  'sequence': 'הקורונה לקחה את הילדים ולנו לא נשאר דבר.',\n",
              "  'token': 3096,\n",
              "  'token_str': 'הילדים'},\n",
              " {'score': 0.04530879110097885,\n",
              "  'sequence': 'הקורונה לקחה את הכסף ולנו לא נשאר דבר.',\n",
              "  'token': 5289,\n",
              "  'token_str': 'הכסף'},\n",
              " {'score': 0.0362359881401062,\n",
              "  'sequence': 'הקורונה לקחה את הכלב ולנו לא נשאר דבר.',\n",
              "  'token': 12737,\n",
              "  'token_str': 'הכלב'},\n",
              " {'score': 0.035021472722291946,\n",
              "  'sequence': 'הקורונה לקחה את הילדה ולנו לא נשאר דבר.',\n",
              "  'token': 12178,\n",
              "  'token_str': 'הילדה'},\n",
              " {'score': 0.02997061051428318,\n",
              "  'sequence': 'הקורונה לקחה את הרכב ולנו לא נשאר דבר.',\n",
              "  'token': 3806,\n",
              "  'token_str': 'הרכב'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HerBKvvPlgnp",
        "outputId": "6387c4b9-1575-4e24-e014-4a3f1b1e6306"
      },
      "source": [
        "max(options, key=lambda x: x[\"score\"])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.14047907292842865,\n",
              " 'sequence': 'הקורונה לקחה את הילדים ולנו לא נשאר דבר.',\n",
              " 'token': 3096,\n",
              " 'token_str': 'הילדים'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}